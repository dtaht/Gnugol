#+TITLE:     GNUGOLD's RATIONALE
#+AUTHOR:    Dave TÃ¤ht
#+EMAIL:     d + gnugol AT taht.net
#+DATE:      <2010-12-19 Sun>
#+LANGUAGE:  en
#+TEXT:      Searching the Web in Plain Text
#+OPTIONS:   H:3 num:t toc:t \n:nil @:t ::t |:t ^:t -:t f:t *:t TeX:t LaTeX:nil skip:nil d:t tags:not-in-toc
#+INFOJS_OPT: view:nil toc:t ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+LINK_UP:
#+LINK_HOME:
#+STYLE:    <link rel="stylesheet" type="text/css" href="stylesheet.css" />
* Why search in plain text?
  In the race to add feature after feature after freature, google has lost sight of the original appeal of the service, simplicity. A typical google page is now over 25K long, not counting the image, contains dozens of extra links, requires a bunch of javascript, and takes 5-10 seconds to download and render over slow, international links. 

I've always admired the DNS system, and felt that now that "search" is almost a commodity, that it would be possible to define a binary udp based protocol for search. There are plenty of unused, useless services in /etc/services - having a special port number for search makes sense.

Problem with that was that most query-response protocols don't work well through a firewall. 

Enter IPv6. Without NAT, you have end to end networking. Response problem solved. Billing problem solved, too.

The first version of this system, in 2008, used 2 packets to get a query out and a response. If you went through a 6in4 tunnel, it's 4 packets. (2 very short ones, however)

This means that by the time you complete a tcp handshake vs the normal google, you've already got a response from this, on a long latency link. 

The earliest prototype of gnugold came in at *half* the time to transmit a query from australia and get the response that google.com.au  did, best case. Due to the unreliable network I was on, it was actually often 5-10 times faster. I began to believe I was onto something.

38 packets exchanged, vs 4. Not bad. Some additional tweaking was in order.

The second nice thing about end to end networking is that it makes it possible -assuming a static ip address - to have for-pay search. I'd gladly pay a few bucks a month for faster - and ad-free searching!

Benefits - vastly reduced data traffic. Control of the formatting can be controlled on the phone or remote servers. 

However, since then, the client rotted, and the protocol was, shall we say, not the best. I figure if I make a usable client that eventually I can go back to what was interesting in the first place - improving search speed through new network protocols - and have thus focused on producing a usable libcurl-based client first, leaving  a new network backend for later, if ever.
